{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation and Benchmarking\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook teaches you how to validate framework outputs against reference data (like MPM systems) and benchmark framework performance. You'll learn validation concepts, benchmarking techniques, MPM comparison workflows, accuracy validation, and statistical validation methods using a unified interactive interface with real-time progress tracking and detailed logging.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- ‚úÖ Understand validation concepts and their importance in AM-QADF\n",
    "- ‚úÖ Benchmark framework operations for performance analysis\n",
    "- ‚úÖ Compare framework outputs with Melt Pool Monitoring (MPM) systems\n",
    "- ‚úÖ Perform accuracy validation against ground truth data\n",
    "- ‚úÖ Conduct statistical validation using hypothesis testing\n",
    "- ‚úÖ Generate comprehensive validation reports\n",
    "- ‚úÖ Interpret validation results and make data-driven decisions\n",
    "- ‚úÖ Monitor validation progress with real-time status and logs\n",
    "\n",
    "## Estimated Duration\n",
    "\n",
    "90-120 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "Validation and benchmarking are critical for ensuring framework reliability and performance. The AM-QADF validation module provides:\n",
    "\n",
    "- ‚è±Ô∏è **Performance Benchmarking**: Measure execution time, memory usage, and throughput\n",
    "- üî¨ **MPM Comparison**: Compare framework outputs with Melt Pool Monitoring systems\n",
    "- üéØ **Accuracy Validation**: Validate against ground truth with RMSE, MAE, R¬≤ metrics\n",
    "- üìä **Statistical Validation**: Hypothesis testing, correlation analysis, significance tests\n",
    "- üìà **Comprehensive Reports**: Generate detailed validation reports with visualizations\n",
    "- üìä **Real-Time Monitoring**: Track progress with status bars and detailed execution logs\n",
    "- ‚è±Ô∏è **Time Tracking**: Monitor execution time for all validation operations\n",
    "\n",
    "The notebook features a unified interactive interface with:\n",
    "- **Progress Tracking**: Visual progress bars showing completion percentage\n",
    "- **Status Monitoring**: Real-time status updates with elapsed time\n",
    "- **Detailed Logging**: Timestamped logs with success/warning/error indicators for all operations\n",
    "- **Error Handling**: Comprehensive error messages and tracebacks in the logs\n",
    "\n",
    "Use the interactive widgets below to validate and benchmark - no coding required! Monitor your validation progress in real-time using the status bar and logs section at the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment variables loaded from development.env\n",
      "‚úÖ Validation classes available\n",
      "‚úÖ Quality assessment client with validation available\n",
      "‚úÖ Connected to MongoDB: am_qadf_data\n",
      "‚úÖ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup: Import required libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory and src directory to path for imports\n",
    "notebook_dir = Path().resolve()\n",
    "project_root = notebook_dir.parent\n",
    "src_dir = project_root / 'src'\n",
    "\n",
    "# Add project root to path (for src.infrastructure imports)\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Add src directory to path (for am_qadf imports)\n",
    "if str(src_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(src_dir))\n",
    "\n",
    "# Core imports\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import (\n",
    "    VBox, HBox, Accordion, Tab, Dropdown, RadioButtons, \n",
    "    Checkbox, Button, Output, Text, IntSlider, FloatSlider,\n",
    "    Layout, Box, Label, FloatText, IntText, SelectMultiple,\n",
    "    HTML as WidgetHTML, Textarea, FileUpload\n",
    ")\n",
    "from IPython.display import display, Markdown, HTML, clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "from typing import Optional, Tuple, Dict, Any, List\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load environment variables from development.env\n",
    "import os\n",
    "env_file = project_root / 'development.env'\n",
    "if env_file.exists():\n",
    "    with open(env_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#') and '=' in line:\n",
    "                key, value = line.split('=', 1)\n",
    "                value = value.strip('\"\\'')\n",
    "                os.environ[key] = value\n",
    "    print(\"‚úÖ Environment variables loaded from development.env\")\n",
    "\n",
    "# Try to import validation classes\n",
    "VALIDATION_AVAILABLE = False\n",
    "validation_client = None\n",
    "quality_client = None\n",
    "\n",
    "try:\n",
    "    from am_qadf.validation import (\n",
    "        ValidationClient, ValidationConfig,\n",
    "        PerformanceBenchmarker, BenchmarkResult,\n",
    "        MPMComparisonEngine, MPMComparisonResult,\n",
    "        AccuracyValidator, AccuracyValidationResult,\n",
    "        StatisticalValidator, StatisticalValidationResult\n",
    "    )\n",
    "    from am_qadf.validation.benchmarking import benchmark\n",
    "    VALIDATION_AVAILABLE = True\n",
    "    print(\"‚úÖ Validation classes available\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Validation classes not available: {e} - using demo mode\")\n",
    "\n",
    "# Try to import quality assessment client\n",
    "try:\n",
    "    from am_qadf.quality.quality_assessment_client import QualityAssessmentClient\n",
    "    quality_client = QualityAssessmentClient(enable_validation=VALIDATION_AVAILABLE)\n",
    "    print(\"‚úÖ Quality assessment client with validation available\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Quality assessment client not available: {e}\")\n",
    "\n",
    "# MongoDB connection setup (optional, for loading real data)\n",
    "INFRASTRUCTURE_AVAILABLE = False\n",
    "mongo_client = None\n",
    "voxel_storage = None\n",
    "\n",
    "try:\n",
    "    from src.infrastructure.config import MongoDBConfig\n",
    "    from src.infrastructure.database import MongoDBClient\n",
    "    from am_qadf.voxel_domain import VoxelGridStorage\n",
    "    \n",
    "    config = MongoDBConfig.from_env()\n",
    "    if not config.username:\n",
    "        config.username = os.getenv('MONGO_ROOT_USERNAME', 'admin')\n",
    "    if not config.password:\n",
    "        config.password = os.getenv('MONGO_ROOT_PASSWORD', 'password')\n",
    "    \n",
    "    mongo_client = MongoDBClient(config=config)\n",
    "    if mongo_client.is_connected():\n",
    "        voxel_storage = VoxelGridStorage(mongo_client=mongo_client)\n",
    "        INFRASTRUCTURE_AVAILABLE = True\n",
    "        print(f\"‚úÖ Connected to MongoDB: {config.database}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è MongoDB connection failed - using demo mode\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è MongoDB not available: {e} - using demo mode\")\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Validation and Benchmarking Interface\n",
    "\n",
    "Use the widgets below to validate framework outputs, benchmark performance, compare with MPM systems, and perform statistical validation. All validation tasks are organized systematically in one unified interface!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c2b12828e144fb87e7a775e4ea4b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HBox(children=(HTML(value='<b>Validation Type:</b>'), RadioButtons(description='‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Interactive Validation and Benchmarking Interface\n",
    "\n",
    "# Global state\n",
    "validation_results = {}\n",
    "benchmark_results = {}\n",
    "mpm_comparison_results = {}\n",
    "accuracy_results = {}\n",
    "statistical_results = {}\n",
    "current_validation_type = None\n",
    "\n",
    "# ============================================\n",
    "# Helper Functions for Demo Data Generation\n",
    "# ============================================\n",
    "\n",
    "def generate_demo_framework_metrics():\n",
    "    \"\"\"Generate demo framework quality metrics.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    return {\n",
    "        'overall_quality_score': 0.90,\n",
    "        'data_quality_score': 0.85,\n",
    "        'signal_quality_score': 0.92,\n",
    "        'alignment_score': 0.88,\n",
    "        'completeness_score': 0.95,\n",
    "        'completeness': 0.90,\n",
    "        'snr': 25.5,\n",
    "        'alignment_accuracy': 0.95,\n",
    "    }\n",
    "\n",
    "def generate_demo_mpm_metrics():\n",
    "    \"\"\"Generate demo MPM quality metrics.\"\"\"\n",
    "    np.random.seed(43)\n",
    "    return {\n",
    "        'overall_quality_score': 0.88,\n",
    "        'data_quality_score': 0.83,\n",
    "        'signal_quality_score': 0.90,\n",
    "        'alignment_score': 0.86,\n",
    "        'completeness_score': 0.93,\n",
    "        'completeness': 0.88,\n",
    "        'snr': 24.8,\n",
    "        'alignment_accuracy': 0.94,\n",
    "    }\n",
    "\n",
    "def generate_demo_ground_truth_signal(shape=(50, 50, 10), noise_level=0.0):\n",
    "    \"\"\"Generate demo ground truth signal.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    signal = np.zeros(shape)\n",
    "    if len(shape) == 3:\n",
    "        z_coords = np.arange(shape[2])[:, np.newaxis, np.newaxis]\n",
    "        y_coords = np.arange(shape[1])[np.newaxis, :, np.newaxis]\n",
    "        x_coords = np.arange(shape[0])[np.newaxis, np.newaxis, :]\n",
    "        signal = 100 + 10 * np.sin(x_coords * 0.1) + 10 * np.cos(y_coords * 0.1) + 5 * np.sin(z_coords * 0.2)\n",
    "    if noise_level > 0:\n",
    "        signal += np.random.normal(0, noise_level, shape)\n",
    "    return signal\n",
    "\n",
    "def generate_demo_framework_signal(ground_truth, noise_level=0.05):\n",
    "    \"\"\"Generate demo framework signal with some error.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    error = np.random.normal(0, np.std(ground_truth) * noise_level, ground_truth.shape)\n",
    "    return ground_truth + error\n",
    "\n",
    "def generate_demo_coordinates(n_points=1000):\n",
    "    \"\"\"Generate demo coordinate arrays.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    ground_truth = np.random.rand(n_points, 3) * 10.0\n",
    "    framework = ground_truth + np.random.normal(0, 0.01, (n_points, 3))\n",
    "    return ground_truth, framework\n",
    "\n",
    "# ============================================\n",
    "# Top Panel: Validation Type Selection and Actions\n",
    "# ============================================\n",
    "\n",
    "validation_type_label = WidgetHTML(\"<b>Validation Type:</b>\")\n",
    "validation_type = RadioButtons(\n",
    "    options=[\n",
    "        ('Performance Benchmarking', 'benchmarking'),\n",
    "        ('MPM Comparison', 'mpm'),\n",
    "        ('Accuracy Validation', 'accuracy'),\n",
    "        ('Statistical Validation', 'statistical'),\n",
    "        ('Comprehensive Workflow', 'comprehensive')\n",
    "    ],\n",
    "    value='benchmarking',\n",
    "    description='Type:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "data_source_label = WidgetHTML(\"<b>Data Source:</b>\")\n",
    "data_source_mode = RadioButtons(\n",
    "    options=[('Demo Data', 'demo'), ('MongoDB', 'mongodb')],\n",
    "    value='demo',\n",
    "    description='Source:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "execute_button = Button(\n",
    "    description='Execute Validation',\n",
    "    button_style='success',\n",
    "    icon='check',\n",
    "    layout=Layout(width='180px')\n",
    ")\n",
    "\n",
    "export_button = Button(\n",
    "    description='Export Report',\n",
    "    button_style='',\n",
    "    icon='download',\n",
    "    layout=Layout(width='150px')\n",
    ")\n",
    "\n",
    "top_panel = VBox([\n",
    "    HBox([validation_type_label, validation_type]),\n",
    "    HBox([data_source_label, data_source_mode, execute_button, export_button])\n",
    "], layout=Layout(padding='10px', border='1px solid #ccc'))\n",
    "\n",
    "# ============================================\n",
    "# Left Panel: Configuration Accordion\n",
    "# ============================================\n",
    "\n",
    "# 1. Benchmarking Configuration\n",
    "benchmarking_label = WidgetHTML(\"<b>Benchmarking Configuration:</b>\")\n",
    "benchmark_operation = Dropdown(\n",
    "    options=[\n",
    "        ('Quality Assessment', 'quality_assessment'),\n",
    "        ('Signal Mapping', 'signal_mapping'),\n",
    "        ('Data Fusion', 'data_fusion'),\n",
    "        ('Query Operation', 'query')\n",
    "    ],\n",
    "    value='quality_assessment',\n",
    "    description='Operation:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "benchmark_data_size = Dropdown(\n",
    "    options=[('Small', 'small'), ('Medium', 'medium'), ('Large', 'large')],\n",
    "    value='medium',\n",
    "    description='Data Size:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "benchmark_iterations = IntSlider(\n",
    "    value=5,\n",
    "    min=1,\n",
    "    max=50,\n",
    "    step=1,\n",
    "    description='Iterations:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "benchmark_warmup = IntSlider(\n",
    "    value=2,\n",
    "    min=0,\n",
    "    max=10,\n",
    "    step=1,\n",
    "    description='Warmup:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "benchmarking_config = VBox([\n",
    "    benchmarking_label,\n",
    "    benchmark_operation,\n",
    "    benchmark_data_size,\n",
    "    benchmark_iterations,\n",
    "    benchmark_warmup\n",
    "], layout=Layout(padding='5px', border='1px solid #ddd'))\n",
    "\n",
    "# 2. MPM Comparison Configuration\n",
    "mpm_label = WidgetHTML(\"<b>MPM Comparison Configuration:</b>\")\n",
    "mpm_correlation_threshold = FloatSlider(\n",
    "    value=0.85,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.05,\n",
    "    description='Correlation Threshold:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "mpm_max_error = FloatSlider(\n",
    "    value=0.1,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description='Max Relative Error:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "mpm_metrics_select = SelectMultiple(\n",
    "    options=['overall_quality_score', 'data_quality_score', 'signal_quality_score', \n",
    "             'alignment_score', 'completeness_score', 'completeness', 'snr', 'alignment_accuracy'],\n",
    "    value=['overall_quality_score', 'completeness', 'snr'],\n",
    "    description='Metrics:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "mpm_config = VBox([\n",
    "    mpm_label,\n",
    "    mpm_correlation_threshold,\n",
    "    mpm_max_error,\n",
    "    mpm_metrics_select\n",
    "], layout=Layout(padding='5px', border='1px solid #ddd'))\n",
    "\n",
    "# 3. Accuracy Validation Configuration\n",
    "accuracy_label = WidgetHTML(\"<b>Accuracy Validation Configuration:</b>\")\n",
    "accuracy_type = RadioButtons(\n",
    "    options=[\n",
    "        ('Signal Mapping', 'signal_mapping'),\n",
    "        ('Spatial Alignment', 'spatial'),\n",
    "        ('Temporal Alignment', 'temporal'),\n",
    "        ('Quality Metrics', 'quality')\n",
    "    ],\n",
    "    value='signal_mapping',\n",
    "    description='Type:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "accuracy_max_error = FloatSlider(\n",
    "    value=0.1,\n",
    "    min=0.01,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description='Max Acceptable Error:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "accuracy_tolerance = FloatSlider(\n",
    "    value=5.0,\n",
    "    min=0.0,\n",
    "    max=20.0,\n",
    "    step=0.5,\n",
    "    description='Tolerance (%):',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "accuracy_config = VBox([\n",
    "    accuracy_label,\n",
    "    accuracy_type,\n",
    "    accuracy_max_error,\n",
    "    accuracy_tolerance\n",
    "], layout=Layout(padding='5px', border='1px solid #ddd'))\n",
    "\n",
    "# 4. Statistical Validation Configuration\n",
    "statistical_label = WidgetHTML(\"<b>Statistical Validation Configuration:</b>\")\n",
    "statistical_test = Dropdown(\n",
    "    options=[\n",
    "        ('T-test', 't_test'),\n",
    "        ('Mann-Whitney U', 'mann_whitney'),\n",
    "        ('Correlation Test', 'correlation'),\n",
    "        ('ANOVA', 'anova'),\n",
    "        ('Normality Test', 'normality')\n",
    "    ],\n",
    "    value='t_test',\n",
    "    description='Test:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "statistical_significance = FloatSlider(\n",
    "    value=0.05,\n",
    "    min=0.001,\n",
    "    max=0.1,\n",
    "    step=0.001,\n",
    "    description='Significance Level (Œ±):',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "statistical_alternative = RadioButtons(\n",
    "    options=[('Two-sided', 'two-sided'), ('Less', 'less'), ('Greater', 'greater')],\n",
    "    value='two-sided',\n",
    "    description='Alternative:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "statistical_config = VBox([\n",
    "    statistical_label,\n",
    "    statistical_test,\n",
    "    statistical_significance,\n",
    "    statistical_alternative\n",
    "], layout=Layout(padding='5px', border='1px solid #ddd'))\n",
    "\n",
    "# Combine into Accordion\n",
    "config_accordion = Accordion(children=[\n",
    "    benchmarking_config,\n",
    "    mpm_config,\n",
    "    accuracy_config,\n",
    "    statistical_config\n",
    "])\n",
    "\n",
    "config_accordion.set_title(0, '‚è±Ô∏è Benchmarking')\n",
    "config_accordion.set_title(1, 'üî¨ MPM Comparison')\n",
    "config_accordion.set_title(2, 'üéØ Accuracy Validation')\n",
    "config_accordion.set_title(3, 'üìä Statistical Validation')\n",
    "\n",
    "left_panel = VBox([\n",
    "    WidgetHTML(\"<h3>Validation Configuration</h3>\"),\n",
    "    config_accordion\n",
    "], layout=Layout(width='300px', padding='10px', border='1px solid #ccc'))\n",
    "\n",
    "# ============================================\n",
    "# Center Panel: Visualization and Results\n",
    "# ============================================\n",
    "\n",
    "viz_mode = RadioButtons(\n",
    "    options=[\n",
    "        ('Benchmark Results', 'benchmark'),\n",
    "        ('MPM Comparison', 'mpm'),\n",
    "        ('Accuracy Metrics', 'accuracy'),\n",
    "        ('Statistical Tests', 'statistical'),\n",
    "        ('Comprehensive Report', 'report')\n",
    "    ],\n",
    "    value='benchmark',\n",
    "    description='View:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "main_output = Output(layout=Layout(height='600px', overflow='auto'))\n",
    "\n",
    "center_panel = VBox([\n",
    "    WidgetHTML(\"<h3>Validation Results</h3>\"),\n",
    "    viz_mode,\n",
    "    main_output\n",
    "], layout=Layout(flex='1 1 auto', padding='10px', border='1px solid #ccc'))\n",
    "\n",
    "# ============================================\n",
    "# Right Panel: Status and Summary\n",
    "# ============================================\n",
    "\n",
    "status_label = WidgetHTML(\"<b>Status:</b>\")\n",
    "status_display = WidgetHTML(\"Ready to validate\")\n",
    "status_section = VBox([\n",
    "    status_label,\n",
    "    status_display\n",
    "], layout=Layout(padding='5px', border='2px solid #4CAF50'))\n",
    "\n",
    "results_summary_label = WidgetHTML(\"<b>Results Summary:</b>\")\n",
    "results_summary_display = WidgetHTML(\"No validation executed yet\")\n",
    "results_summary_section = VBox([\n",
    "    results_summary_label,\n",
    "    results_summary_display\n",
    "], layout=Layout(padding='5px'))\n",
    "\n",
    "metrics_display_label = WidgetHTML(\"<b>Key Metrics:</b>\")\n",
    "metrics_display = WidgetHTML(\"No metrics available\")\n",
    "metrics_section = VBox([\n",
    "    metrics_display_label,\n",
    "    metrics_display\n",
    "], layout=Layout(padding='5px'))\n",
    "\n",
    "validation_status_label = WidgetHTML(\"<b>Validation Status:</b>\")\n",
    "validation_status_display = WidgetHTML(\"Not validated\")\n",
    "validation_status_section = VBox([\n",
    "    validation_status_label,\n",
    "    validation_status_display\n",
    "], layout=Layout(padding='5px'))\n",
    "\n",
    "right_panel = VBox([\n",
    "    status_section,\n",
    "    results_summary_section,\n",
    "    metrics_section,\n",
    "    validation_status_section\n",
    "], layout=Layout(width='250px', padding='10px', border='1px solid #ccc'))\n",
    "\n",
    "# ============================================\n",
    "# Execute Validation Function\n",
    "# ============================================\n",
    "\n",
    "def execute_validation(b):\n",
    "    \"\"\"Execute validation based on selected type.\"\"\"\n",
    "    global operation_start_time\n",
    "    operation_start_time = time.time()\n",
    "    \n",
    "    # Clear logs\n",
    "    with validation_logs:\n",
    "        clear_output(wait=True)\n",
    "        display(HTML(\"<p><i>Validation logs will appear here...</i></p>\"))\n",
    "    \n",
    "    with main_output:\n",
    "        clear_output(wait=True)\n",
    "        val_type = validation_type.value\n",
    "        \n",
    "        log_message(f\"Starting {validation_type.label}...\", 'info')\n",
    "        update_status(f\"Executing {validation_type.label}...\", 0)\n",
    "        \n",
    "        try:\n",
    "            if val_type == 'benchmarking':\n",
    "                execute_benchmarking()\n",
    "            elif val_type == 'mpm':\n",
    "                execute_mpm_comparison()\n",
    "            elif val_type == 'accuracy':\n",
    "                execute_accuracy_validation()\n",
    "            elif val_type == 'statistical':\n",
    "                execute_statistical_validation()\n",
    "            elif val_type == 'comprehensive':\n",
    "                execute_comprehensive_workflow()\n",
    "            \n",
    "            log_message(f\"{validation_type.label} completed successfully\", 'success')\n",
    "            update_status(f\"{validation_type.label} complete\", 100)\n",
    "        except Exception as e:\n",
    "            log_message(f\"Error during {validation_type.label}: {str(e)}\", 'error')\n",
    "            import traceback\n",
    "            log_message(f\"Traceback: {traceback.format_exc()}\", 'error')\n",
    "            warning_display.value = f\"<span style='color: red;'>‚ùå Error: {str(e)}</span>\"\n",
    "            update_status(f\"Error during {validation_type.label}\", 0)\n",
    "\n",
    "def execute_benchmarking():\n",
    "    \"\"\"Execute performance benchmarking.\"\"\"\n",
    "    log_message(\"Performance Benchmarking\", 'info')\n",
    "    log_message(f\"Operation: {benchmark_operation.label}\", 'info')\n",
    "    log_message(f\"Data Size: {benchmark_data_size.value}\", 'info')\n",
    "    log_message(f\"Iterations: {benchmark_iterations.value}, Warmup: {benchmark_warmup.value}\", 'info')\n",
    "    update_status(\"Initializing benchmark...\", 10)\n",
    "    \n",
    "    print(\"‚è±Ô∏è Performance Benchmarking\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Operation: {benchmark_operation.label}\")\n",
    "    print(f\"Data Size: {benchmark_data_size.value}\")\n",
    "    print(f\"Iterations: {benchmark_iterations.value}\")\n",
    "    print(f\"Warmup: {benchmark_warmup.value}\")\n",
    "    print()\n",
    "    \n",
    "    # Size mapping\n",
    "    size_map = {'small': (20, 20, 5), 'medium': (50, 50, 10), 'large': (100, 100, 20)}\n",
    "    shape = size_map[benchmark_data_size.value]\n",
    "    \n",
    "    def demo_operation():\n",
    "        data = np.random.rand(*shape) * 100\n",
    "        time.sleep(0.01 * (np.prod(shape) / 25000))\n",
    "        return np.mean(data)\n",
    "    \n",
    "    if VALIDATION_AVAILABLE:\n",
    "        try:\n",
    "            update_status(\"Running benchmark iterations...\", 30)\n",
    "            log_message(\"Creating PerformanceBenchmarker...\", 'info')\n",
    "            benchmarker = PerformanceBenchmarker()\n",
    "            \n",
    "            log_message(\"Executing benchmark...\", 'info')\n",
    "            result = benchmarker.benchmark_operation(\n",
    "                benchmark_operation.value,\n",
    "                demo_operation,\n",
    "                iterations=benchmark_iterations.value,\n",
    "                warmup_iterations=benchmark_warmup.value\n",
    "            )\n",
    "            benchmark_results['latest'] = result\n",
    "            \n",
    "            update_status(\"Benchmark complete\", 80)\n",
    "            log_message(f\"Average execution time: {result.execution_time:.4f}s\", 'success')\n",
    "            log_message(f\"Memory usage: {result.memory_usage:.2f} MB\", 'info')\n",
    "            log_message(f\"Throughput: {result.throughput:.2f} elements/s\", 'info')\n",
    "            \n",
    "            print(\"‚úÖ Benchmark Complete!\")\n",
    "            print(f\"\\nüìä Results:\")\n",
    "            print(f\"   Average Time: {result.execution_time:.4f}s\")\n",
    "            print(f\"   Memory Usage: {result.memory_usage:.2f} MB\")\n",
    "            print(f\"   Throughput: {result.throughput:.2f} elements/s\")\n",
    "            \n",
    "            # Visualization\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            if hasattr(result, 'metadata') and 'min_time' in result.metadata:\n",
    "                times = [result.metadata['min_time'], result.execution_time, result.metadata['max_time']]\n",
    "                labels = ['Min', 'Average', 'Max']\n",
    "                colors = ['green', 'blue', 'red']\n",
    "                ax.bar(labels, times, color=colors, alpha=0.7)\n",
    "                ax.set_ylabel('Time (seconds)')\n",
    "                ax.set_title('Benchmark Results')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Update status\n",
    "            status_display.value = f\"‚úÖ Benchmark complete: {result.execution_time:.4f}s\"\n",
    "            results_summary_display.value = f\"Operation: {result.operation_name}<br>Time: {result.execution_time:.4f}s<br>Memory: {result.memory_usage:.2f} MB\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Benchmark failed: {e}\")\n",
    "            status_display.value = f\"‚ùå Error: {e}\"\n",
    "    else:\n",
    "        # Demo mode\n",
    "        log_message(\"Running benchmark in demo mode...\", 'warning')\n",
    "        import time as time_module\n",
    "        times = []\n",
    "        for i in range(benchmark_iterations.value + benchmark_warmup.value):\n",
    "            start = time_module.perf_counter()\n",
    "            demo_operation()\n",
    "            elapsed = time_module.perf_counter() - start\n",
    "            if i >= benchmark_warmup.value:\n",
    "                times.append(elapsed)\n",
    "        \n",
    "        avg_time = np.mean(times)\n",
    "        min_time = np.min(times)\n",
    "        max_time = np.max(times)\n",
    "        \n",
    "        log_message(f\"Average execution time: {avg_time:.4f}s\", 'success')\n",
    "        log_message(f\"Min time: {min_time:.4f}s, Max time: {max_time:.4f}s\", 'info')\n",
    "        \n",
    "        print(\"‚úÖ Benchmark Complete! (Demo Mode)\")\n",
    "        print(f\"\\nüìä Results:\")\n",
    "        print(f\"   Average Time: {avg_time:.4f}s\")\n",
    "        print(f\"   Min Time: {min_time:.4f}s\")\n",
    "        print(f\"   Max Time: {max_time:.4f}s\")\n",
    "        \n",
    "        update_status(\"Benchmark complete (demo)\", 80)\n",
    "        status_display.value = f\"‚úÖ Benchmark complete (demo): {avg_time:.4f}s\"\n",
    "        results_summary_display.value = f\"Benchmark (Demo Mode):<br>Avg Time: {avg_time:.4f}s<br>Min: {min_time:.4f}s<br>Max: {max_time:.4f}s\"\n",
    "\n",
    "def execute_mpm_comparison():\n",
    "    \"\"\"Execute MPM comparison.\"\"\"\n",
    "    log_message(\"MPM Comparison\", 'info')\n",
    "    update_status(\"Generating demo metrics...\", 10)\n",
    "    \n",
    "    framework_metrics = generate_demo_framework_metrics()\n",
    "    mpm_metrics = generate_demo_mpm_metrics()\n",
    "    \n",
    "    log_message(f\"Framework metrics: {len(framework_metrics)} metrics\", 'info')\n",
    "    log_message(f\"MPM metrics: {len(mpm_metrics)} metrics\", 'info')\n",
    "    \n",
    "    print(\"üî¨ MPM Comparison\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"Framework Metrics:\")\n",
    "    for key, value in framework_metrics.items():\n",
    "        print(f\"  {key}: {value:.3f}\")\n",
    "    print()\n",
    "    print(\"MPM Metrics:\")\n",
    "    for key, value in mpm_metrics.items():\n",
    "        print(f\"  {key}: {value:.3f}\")\n",
    "    print()\n",
    "    \n",
    "    if VALIDATION_AVAILABLE:\n",
    "        try:\n",
    "            update_status(\"Creating MPM comparison engine...\", 30)\n",
    "            log_message(f\"Correlation threshold: {mpm_correlation_threshold.value}\", 'info')\n",
    "            log_message(f\"Max relative error: {mpm_max_error.value}\", 'info')\n",
    "            \n",
    "            mpm_comparer = MPMComparisonEngine(\n",
    "                correlation_threshold=mpm_correlation_threshold.value,\n",
    "                max_relative_error=mpm_max_error.value\n",
    "            )\n",
    "            \n",
    "            update_status(\"Comparing metrics...\", 50)\n",
    "            log_message(\"Comparing framework and MPM metrics...\", 'info')\n",
    "            results = mpm_comparer.compare_quality_metrics(framework_metrics, mpm_metrics)\n",
    "            mpm_comparison_results['latest'] = results\n",
    "            \n",
    "            update_status(\"Comparison complete\", 80)\n",
    "            log_message(f\"Compared {len(results)} metrics\", 'success')\n",
    "            \n",
    "            print(\"‚úÖ Comparison Complete!\")\n",
    "            print(\"\\nüìä Comparison Results:\")\n",
    "            \n",
    "            comparison_data = []\n",
    "            for metric_name, result in results.items():\n",
    "                comparison_data.append({\n",
    "                    'Metric': metric_name,\n",
    "                    'Framework': result.framework_value,\n",
    "                    'MPM': result.mpm_value,\n",
    "                    'Difference': result.difference,\n",
    "                    'Relative Error %': result.relative_error,\n",
    "                    'Correlation': result.correlation,\n",
    "                    'Valid': '‚úì' if result.is_valid else '‚úó'\n",
    "                })\n",
    "                \n",
    "                if result.is_valid:\n",
    "                    log_message(f\"Metric '{metric_name}': Valid (corr={result.correlation:.3f}, err={result.relative_error:.2f}%)\", 'success')\n",
    "                else:\n",
    "                    log_message(f\"Metric '{metric_name}': Invalid (corr={result.correlation:.3f}, err={result.relative_error:.2f}%)\", 'warning')\n",
    "            \n",
    "            df = pd.DataFrame(comparison_data)\n",
    "            display(df.style.format({\n",
    "                'Framework': '{:.3f}',\n",
    "                'MPM': '{:.3f}',\n",
    "                'Difference': '{:.4f}',\n",
    "                'Relative Error %': '{:.2f}',\n",
    "                'Correlation': '{:.3f}'\n",
    "            }))\n",
    "            \n",
    "            valid_count = sum(1 for r in results.values() if r.is_valid)\n",
    "            status_display.value = f\"‚úÖ MPM comparison: {valid_count}/{len(results)} metrics valid\"\n",
    "            results_summary_display.value = f\"MPM Comparison:<br>{valid_count}/{len(results)} metrics validated<br>Correlation threshold: {mpm_correlation_threshold.value}\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            log_message(f\"MPM comparison failed: {str(e)}\", 'error')\n",
    "            print(f\"‚ùå MPM comparison failed: {e}\")\n",
    "            status_display.value = f\"‚ùå Error: {e}\"\n",
    "            warning_display.value = f\"<span style='color: red;'>‚ùå Error: {str(e)}</span>\"\n",
    "    else:\n",
    "        log_message(\"Running MPM comparison in demo mode...\", 'warning')\n",
    "        print(\"‚úÖ Comparison Complete! (Demo Mode)\")\n",
    "        update_status(\"MPM comparison complete (demo)\", 80)\n",
    "        status_display.value = \"‚úÖ MPM comparison complete (demo)\"\n",
    "        results_summary_display.value = \"MPM Comparison (Demo Mode):<br>Comparison completed with demo data\"\n",
    "\n",
    "def execute_accuracy_validation():\n",
    "    \"\"\"Execute accuracy validation.\"\"\"\n",
    "    log_message(\"Accuracy Validation\", 'info')\n",
    "    update_status(\"Generating ground truth data...\", 10)\n",
    "    \n",
    "    ground_truth = generate_demo_ground_truth_signal()\n",
    "    framework_signal = generate_demo_framework_signal(ground_truth)\n",
    "    \n",
    "    log_message(f\"Ground truth shape: {ground_truth.shape}\", 'info')\n",
    "    log_message(f\"Framework signal shape: {framework_signal.shape}\", 'info')\n",
    "    \n",
    "    print(\"üéØ Accuracy Validation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if VALIDATION_AVAILABLE:\n",
    "        try:\n",
    "            update_status(\"Creating accuracy validator...\", 30)\n",
    "            log_message(f\"Max acceptable error: {accuracy_max_error.value}\", 'info')\n",
    "            log_message(f\"Tolerance: {accuracy_tolerance.value}%\", 'info')\n",
    "            \n",
    "            validator = AccuracyValidator(\n",
    "                max_acceptable_error=accuracy_max_error.value,\n",
    "                tolerance_percent=accuracy_tolerance.value\n",
    "            )\n",
    "            \n",
    "            update_status(\"Validating signal mapping...\", 50)\n",
    "            log_message(\"Calculating accuracy metrics...\", 'info')\n",
    "            result = validator.validate_signal_mapping(framework_signal, ground_truth, \"demo_signal\")\n",
    "            accuracy_results['latest'] = result\n",
    "            \n",
    "            update_status(\"Accuracy validation complete\", 80)\n",
    "            log_message(f\"RMSE: {result.rmse:.6f}\", 'info')\n",
    "            log_message(f\"MAE: {result.mae:.6f}\", 'info')\n",
    "            log_message(f\"R¬≤ Score: {result.r2_score:.4f}\", 'success' if result.r2_score > 0.9 else 'warning')\n",
    "            log_message(f\"Within tolerance: {'Yes' if result.within_tolerance else 'No'}\", 'success' if result.within_tolerance else 'warning')\n",
    "            \n",
    "            print(\"‚úÖ Validation Complete!\")\n",
    "            print(f\"\\nüìä Accuracy Metrics:\")\n",
    "            print(f\"   RMSE: {result.rmse:.6f}\")\n",
    "            print(f\"   MAE: {result.mae:.6f}\")\n",
    "            print(f\"   R¬≤ Score: {result.r2_score:.4f}\")\n",
    "            print(f\"   Max Error: {result.max_error:.6f}\")\n",
    "            print(f\"   Within Tolerance: {'‚úì Yes' if result.within_tolerance else '‚úó No'}\")\n",
    "            \n",
    "            status_display.value = f\"‚úÖ Accuracy validation: R¬≤ = {result.r2_score:.4f}\"\n",
    "            metrics_display.value = f\"RMSE: {result.rmse:.6f}<br>MAE: {result.mae:.6f}<br>R¬≤: {result.r2_score:.4f}<br>Max Error: {result.max_error:.6f}\"\n",
    "            validation_status_display.value = \"‚úì Validated\" if result.within_tolerance else \"‚úó Outside Tolerance\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            log_message(f\"Accuracy validation failed: {str(e)}\", 'error')\n",
    "            print(f\"‚ùå Accuracy validation failed: {e}\")\n",
    "            status_display.value = f\"‚ùå Error: {e}\"\n",
    "            warning_display.value = f\"<span style='color: red;'>‚ùå Error: {str(e)}</span>\"\n",
    "    else:\n",
    "        log_message(\"Running accuracy validation in demo mode...\", 'warning')\n",
    "        # Calculate demo metrics\n",
    "        errors = (framework_signal - ground_truth).flatten()\n",
    "        rmse = np.sqrt(np.mean(errors**2))\n",
    "        mae = np.mean(np.abs(errors))\n",
    "        r2 = 1 - (np.sum(errors**2) / np.sum((ground_truth.flatten() - np.mean(ground_truth))**2))\n",
    "        max_error = np.max(np.abs(errors))\n",
    "        \n",
    "        log_message(f\"RMSE: {rmse:.6f}, MAE: {mae:.6f}, R¬≤: {r2:.4f}\", 'info')\n",
    "        \n",
    "        print(\"‚úÖ Validation Complete! (Demo Mode)\")\n",
    "        print(f\"\\nüìä Accuracy Metrics (Demo):\")\n",
    "        print(f\"   RMSE: {rmse:.6f}\")\n",
    "        print(f\"   MAE: {mae:.6f}\")\n",
    "        print(f\"   R¬≤ Score: {r2:.4f}\")\n",
    "        print(f\"   Max Error: {max_error:.6f}\")\n",
    "        \n",
    "        update_status(\"Accuracy validation complete (demo)\", 80)\n",
    "        status_display.value = f\"‚úÖ Accuracy validation complete (demo): R¬≤ = {r2:.4f}\"\n",
    "        metrics_display.value = f\"RMSE: {rmse:.6f}<br>MAE: {mae:.6f}<br>R¬≤: {r2:.4f}<br>Max Error: {max_error:.6f}\"\n",
    "\n",
    "def execute_statistical_validation():\n",
    "    \"\"\"Execute statistical validation.\"\"\"\n",
    "    log_message(\"Statistical Validation\", 'info')\n",
    "    update_status(\"Generating test data...\", 10)\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    baseline = np.random.normal(0.85, 0.05, 50)\n",
    "    improved = np.random.normal(0.90, 0.05, 50)\n",
    "    \n",
    "    log_message(f\"Baseline: mean={np.mean(baseline):.3f}, std={np.std(baseline):.3f}\", 'info')\n",
    "    log_message(f\"Improved: mean={np.mean(improved):.3f}, std={np.std(improved):.3f}\", 'info')\n",
    "    \n",
    "    print(\"üìä Statistical Validation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if VALIDATION_AVAILABLE:\n",
    "        try:\n",
    "            update_status(\"Creating statistical validator...\", 30)\n",
    "            log_message(f\"Test: {statistical_test.label}\", 'info')\n",
    "            log_message(f\"Significance level (Œ±): {statistical_significance.value}\", 'info')\n",
    "            log_message(f\"Alternative: {statistical_alternative.value}\", 'info')\n",
    "            \n",
    "            validator = StatisticalValidator(significance_level=statistical_significance.value)\n",
    "            \n",
    "            update_status(\"Performing statistical test...\", 50)\n",
    "            if statistical_test.value == 't_test':\n",
    "                log_message(\"Running t-test...\", 'info')\n",
    "                result = validator.t_test(baseline, improved, alternative=statistical_alternative.value)\n",
    "            elif statistical_test.value == 'correlation':\n",
    "                log_message(\"Running correlation test...\", 'info')\n",
    "                x = np.linspace(0, 10, 50)\n",
    "                y = x + np.random.normal(0, 0.5, 50)\n",
    "                result = validator.correlation_test(x, y)\n",
    "            elif statistical_test.value == 'mann_whitney':\n",
    "                log_message(\"Running Mann-Whitney U test...\", 'info')\n",
    "                result = validator.mann_whitney_u_test(baseline, improved)\n",
    "            elif statistical_test.value == 'anova':\n",
    "                log_message(\"Running ANOVA test...\", 'info')\n",
    "                result = validator.anova_test([baseline, improved])\n",
    "            else:\n",
    "                result = validator.t_test(baseline, improved)\n",
    "            \n",
    "            statistical_results['latest'] = result\n",
    "            \n",
    "            update_status(\"Statistical test complete\", 80)\n",
    "            log_message(f\"Test statistic: {result.test_statistic:.4f}\", 'info')\n",
    "            log_message(f\"P-value: {result.p_value:.6f}\", 'info')\n",
    "            log_message(f\"Significant: {'Yes' if result.is_significant else 'No'} (Œ± = {result.significance_level})\", 'success' if result.is_significant else 'warning')\n",
    "            \n",
    "            print(\"‚úÖ Statistical Test Complete!\")\n",
    "            print(f\"\\nüìä Results:\")\n",
    "            print(f\"   Test: {result.test_name}\")\n",
    "            print(f\"   P-value: {result.p_value:.6f}\")\n",
    "            print(f\"   Significant: {'‚úì Yes' if result.is_significant else '‚úó No'}\")\n",
    "            print(f\"   Conclusion: {result.conclusion}\")\n",
    "            \n",
    "            status_display.value = f\"‚úÖ Statistical test: p = {result.p_value:.4f}\"\n",
    "            metrics_display.value = f\"Test: {result.test_name}<br>P-value: {result.p_value:.6f}<br>Significant: {'Yes' if result.is_significant else 'No'}<br>Statistic: {result.test_statistic:.4f}\"\n",
    "            validation_status_display.value = \"‚úì Significant\" if result.is_significant else \"‚úó Not Significant\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            log_message(f\"Statistical validation failed: {str(e)}\", 'error')\n",
    "            import traceback\n",
    "            log_message(f\"Traceback: {traceback.format_exc()}\", 'error')\n",
    "            print(f\"‚ùå Statistical validation failed: {e}\")\n",
    "            status_display.value = f\"‚ùå Error: {e}\"\n",
    "            warning_display.value = f\"<span style='color: red;'>‚ùå Error: {str(e)}</span>\"\n",
    "    else:\n",
    "        log_message(\"Running statistical test in demo mode...\", 'warning')\n",
    "        try:\n",
    "            from scipy import stats\n",
    "            if statistical_test.value == 't_test':\n",
    "                t_stat, p_value = stats.ttest_ind(baseline, improved)\n",
    "                log_message(f\"T-statistic: {t_stat:.4f}, P-value: {p_value:.6f}\", 'info')\n",
    "            elif statistical_test.value == 'correlation':\n",
    "                x = np.linspace(0, 10, 50)\n",
    "                y = x + np.random.normal(0, 0.5, 50)\n",
    "                corr, p_value = stats.pearsonr(x, y)\n",
    "                t_stat = corr\n",
    "                log_message(f\"Correlation: {corr:.4f}, P-value: {p_value:.6f}\", 'info')\n",
    "            else:\n",
    "                t_stat, p_value = stats.ttest_ind(baseline, improved)\n",
    "                log_message(f\"Demo test - T-statistic: {t_stat:.4f}, P-value: {p_value:.6f}\", 'info')\n",
    "        except ImportError:\n",
    "            # Fallback if scipy not available\n",
    "            p_value = 0.01\n",
    "            t_stat = 2.5\n",
    "            log_message(\"scipy not available, using demo values\", 'warning')\n",
    "        \n",
    "        is_significant = p_value < statistical_significance.value\n",
    "        log_message(f\"Significant: {'Yes' if is_significant else 'No'} (p={p_value:.6f}, Œ±={statistical_significance.value})\", 'success' if is_significant else 'warning')\n",
    "        \n",
    "        print(\"‚úÖ Statistical Test Complete! (Demo Mode)\")\n",
    "        print(f\"\\nüìä T-Test Results (Demo):\")\n",
    "        print(f\"   T-statistic: {t_stat:.4f}\")\n",
    "        print(f\"   P-value: {p_value:.6f}\")\n",
    "        print(f\"   Significant: {'‚úì Yes' if is_significant else '‚úó No'} (Œ± = {statistical_significance.value})\")\n",
    "        \n",
    "        update_status(\"Statistical test complete (demo)\", 80)\n",
    "        status_display.value = f\"‚úÖ Statistical test complete (demo): p = {p_value:.4f}\"\n",
    "        metrics_display.value = f\"Test: {statistical_test.label} (Demo)<br>P-value: {p_value:.6f}<br>Significant: {'Yes' if is_significant else 'No'}<br>Statistic: {t_stat:.4f}\"\n",
    "        validation_status_display.value = \"‚úì Significant\" if is_significant else \"‚úó Not Significant\"\n",
    "\n",
    "def execute_comprehensive_workflow():\n",
    "    \"\"\"Execute comprehensive validation workflow.\"\"\"\n",
    "    log_message(\"Comprehensive Validation Workflow\", 'info')\n",
    "    log_message(\"Running all validation steps in sequence...\", 'info')\n",
    "    \n",
    "    print(\"üöÄ Comprehensive Validation Workflow\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nRunning all validation steps...\\n\")\n",
    "    \n",
    "    # Step 1: Benchmark\n",
    "    update_status(\"Step 1/4: Performance Benchmarking\", 10)\n",
    "    log_message(\"=\" * 60, 'info')\n",
    "    log_message(\"Step 1: Performance Benchmarking\", 'info')\n",
    "    print(\"Step 1: Performance Benchmarking\")\n",
    "    print(\"-\" * 60)\n",
    "    execute_benchmarking()\n",
    "    print()\n",
    "    \n",
    "    # Step 2: MPM Comparison\n",
    "    update_status(\"Step 2/4: MPM Comparison\", 30)\n",
    "    log_message(\"=\" * 60, 'info')\n",
    "    log_message(\"Step 2: MPM Comparison\", 'info')\n",
    "    print(\"Step 2: MPM Comparison\")\n",
    "    print(\"-\" * 60)\n",
    "    execute_mpm_comparison()\n",
    "    print()\n",
    "    \n",
    "    # Step 3: Accuracy Validation\n",
    "    update_status(\"Step 3/4: Accuracy Validation\", 50)\n",
    "    log_message(\"=\" * 60, 'info')\n",
    "    log_message(\"Step 3: Accuracy Validation\", 'info')\n",
    "    print(\"Step 3: Accuracy Validation\")\n",
    "    print(\"-\" * 60)\n",
    "    execute_accuracy_validation()\n",
    "    print()\n",
    "    \n",
    "    # Step 4: Statistical Validation\n",
    "    update_status(\"Step 4/4: Statistical Validation\", 70)\n",
    "    log_message(\"=\" * 60, 'info')\n",
    "    log_message(\"Step 4: Statistical Validation\", 'info')\n",
    "    print(\"Step 4: Statistical Validation\")\n",
    "    print(\"-\" * 60)\n",
    "    execute_statistical_validation()\n",
    "    print()\n",
    "    \n",
    "    # Generate report\n",
    "    update_status(\"Generating comprehensive report...\", 90)\n",
    "    log_message(\"=\" * 60, 'info')\n",
    "    log_message(\"Generating validation report...\", 'info')\n",
    "    \n",
    "    if VALIDATION_AVAILABLE:\n",
    "        try:\n",
    "            v_client = ValidationClient()\n",
    "            all_results = {\n",
    "                'mpm_comparison': mpm_comparison_results.get('latest', {}),\n",
    "                'accuracy': accuracy_results.get('latest'),\n",
    "                'statistical': statistical_results.get('latest'),\n",
    "                'benchmark': benchmark_results.get('latest')\n",
    "            }\n",
    "            report = v_client.generate_validation_report(all_results)\n",
    "            validation_reports['latest'] = report\n",
    "            log_message(f\"Validation report generated ({len(report)} characters)\", 'success')\n",
    "        except Exception as e:\n",
    "            log_message(f\"Report generation: {e}\", 'warning')\n",
    "    \n",
    "    log_message(\"=\" * 60, 'info')\n",
    "    log_message(\"‚úÖ Comprehensive Validation Workflow Complete!\", 'success')\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚úÖ Comprehensive Validation Workflow Complete!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    update_status(\"Comprehensive workflow complete\", 100)\n",
    "    status_display.value = \"‚úÖ Comprehensive workflow complete\"\n",
    "    results_summary_display.value = \"All validation steps completed successfully\"\n",
    "\n",
    "# Connect button\n",
    "execute_button.on_click(execute_validation)\n",
    "\n",
    "# Update view based on validation type\n",
    "def update_view(change):\n",
    "    \"\"\"Update visualization mode based on validation type.\"\"\"\n",
    "    val_type = change['new']\n",
    "    if val_type == 'benchmarking':\n",
    "        viz_mode.value = 'benchmark'\n",
    "    elif val_type == 'mpm':\n",
    "        viz_mode.value = 'mpm'\n",
    "    elif val_type == 'accuracy':\n",
    "        viz_mode.value = 'accuracy'\n",
    "    elif val_type == 'statistical':\n",
    "        viz_mode.value = 'statistical'\n",
    "    elif val_type == 'comprehensive':\n",
    "        viz_mode.value = 'report'\n",
    "\n",
    "validation_type.observe(update_view, names='value')\n",
    "\n",
    "# ============================================\n",
    "# Export Button Functionality\n",
    "# ============================================\n",
    "\n",
    "def export_report(b):\n",
    "    \"\"\"Export validation report.\"\"\"\n",
    "    if not validation_reports.get('latest'):\n",
    "        warning_display.value = \"<span style='color: orange;'>‚ö†Ô∏è No validation report available. Run validation first.</span>\"\n",
    "        log_message(\"Export attempted but no report available\", 'warning')\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        report = validation_reports['latest']\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"validation_report_{timestamp}.txt\"\n",
    "        \n",
    "        # In a real implementation, this would download the file\n",
    "        # For now, we'll display it and log\n",
    "        log_message(f\"Report ready for export: {filename} ({len(report)} characters)\", 'info')\n",
    "        warning_display.value = f\"<span style='color: green;'>‚úÖ Report ready: {filename}</span>\"\n",
    "        \n",
    "        # Display first part of report\n",
    "        with main_output:\n",
    "            clear_output(wait=True)\n",
    "            print(\"üìÑ Validation Report\")\n",
    "            print(\"=\" * 60)\n",
    "            print(report[:3000] + \"...\" if len(report) > 3000 else report)\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Export failed: {str(e)}\", 'error')\n",
    "        warning_display.value = f\"<span style='color: red;'>‚ùå Export failed: {str(e)}</span>\"\n",
    "\n",
    "export_button.on_click(export_report)\n",
    "\n",
    "# ============================================\n",
    "# Bottom Panel: Status, Progress, and Logs\n",
    "# ============================================\n",
    "\n",
    "# Current operation status\n",
    "current_operation = WidgetHTML(value='<b>Status:</b> Ready to validate')\n",
    "\n",
    "# Progress bar\n",
    "progress_bar = widgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    description='Progress:',\n",
    "    bar_style='info',\n",
    "    layout=Layout(width='100%')\n",
    ")\n",
    "\n",
    "# Validation logs output\n",
    "validation_logs = Output(layout=Layout(height='200px', border='1px solid #ccc', overflow_y='auto'))\n",
    "\n",
    "# Initialize logs\n",
    "with validation_logs:\n",
    "    display(HTML(\"<p><i>Validation logs will appear here...</i></p>\"))\n",
    "\n",
    "# Bottom status bar (shows Status | Progress | Time)\n",
    "bottom_status = WidgetHTML(value='<b>Status:</b> Ready | <b>Progress:</b> 0% | <b>Time:</b> 0:00')\n",
    "bottom_progress = widgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    description='Overall:',\n",
    "    bar_style='info',\n",
    "    layout=Layout(width='100%')\n",
    ")\n",
    "\n",
    "# Warning display\n",
    "warning_display = WidgetHTML(\"\")\n",
    "\n",
    "# Global time tracking\n",
    "operation_start_time = None\n",
    "\n",
    "# Bottom panel\n",
    "bottom_panel = VBox([\n",
    "    current_operation,\n",
    "    progress_bar,\n",
    "    WidgetHTML(\"<b>Validation Logs:</b>\"),\n",
    "    validation_logs,\n",
    "    WidgetHTML(\"<hr>\"),\n",
    "    bottom_status,\n",
    "    bottom_progress,\n",
    "    warning_display\n",
    "], layout=Layout(padding='10px', border='1px solid #ccc'))\n",
    "\n",
    "# ============================================\n",
    "# Logging Functions\n",
    "# ============================================\n",
    "\n",
    "def log_message(message: str, level: str = 'info'):\n",
    "    \"\"\"Log a message to the validation logs with timestamp and emoji.\"\"\"\n",
    "    timestamp = datetime.now().strftime('%H:%M:%S')\n",
    "    icons = {'info': '‚ÑπÔ∏è', 'success': '‚úÖ', 'warning': '‚ö†Ô∏è', 'error': '‚ùå'}\n",
    "    icon = icons.get(level, '‚ÑπÔ∏è')\n",
    "    with validation_logs:\n",
    "        print(f\"[{timestamp}] {icon} {message}\")\n",
    "\n",
    "def update_status(operation: str, progress: int = None):\n",
    "    \"\"\"Update the status display and progress.\"\"\"\n",
    "    global operation_start_time\n",
    "    current_operation.value = f'<b>Status:</b> {operation}'\n",
    "    if progress is not None:\n",
    "        progress_bar.value = progress\n",
    "        bottom_progress.value = progress\n",
    "        if operation_start_time:\n",
    "            elapsed = time.time() - operation_start_time\n",
    "            bottom_status.value = f'<b>Status:</b> {operation} | <b>Progress:</b> {progress}% | <b>Time:</b> {time.strftime(\"%M:%S\", time.gmtime(elapsed))}'\n",
    "        else:\n",
    "            bottom_status.value = f'<b>Status:</b> {operation} | <b>Progress:</b> {progress}% | <b>Time:</b> 0:00'\n",
    "    else:\n",
    "        if operation_start_time:\n",
    "            elapsed = time.time() - operation_start_time\n",
    "            bottom_status.value = f'<b>Status:</b> {operation} | <b>Progress:</b> {progress_bar.value}% | <b>Time:</b> {time.strftime(\"%M:%S\", time.gmtime(elapsed))}'\n",
    "        else:\n",
    "            bottom_status.value = f'<b>Status:</b> {operation} | <b>Progress:</b> {progress_bar.value}% | <b>Time:</b> 0:00'\n",
    "\n",
    "# ============================================\n",
    "# Complete Interface Layout\n",
    "# ============================================\n",
    "\n",
    "main_layout = VBox([\n",
    "    top_panel,\n",
    "    HBox([left_panel, center_panel, right_panel], layout=Layout(width='100%', border='2px solid #333', padding='10px')),\n",
    "    bottom_panel\n",
    "])\n",
    "\n",
    "display(main_layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "You've learned how to:\n",
    "\n",
    "1. **Benchmark Framework Operations**: Measure performance metrics (execution time, memory, throughput) with detailed progress tracking\n",
    "2. **Compare with MPM Systems**: Validate framework outputs against Melt Pool Monitoring reference data\n",
    "3. **Validate Accuracy**: Calculate error metrics (RMSE, MAE, R¬≤) against ground truth with real-time status updates\n",
    "4. **Perform Statistical Tests**: Use hypothesis testing to validate improvements and significance\n",
    "5. **Generate Comprehensive Reports**: Combine all validation results into detailed reports\n",
    "6. **Monitor Validation Progress**: Use the status bar and logs section to track validation operations in real-time\n",
    "7. **Interpret Logs**: Understand timestamped log messages with success/warning/error indicators\n",
    "\n",
    "### Interface Features\n",
    "\n",
    "The notebook provides a comprehensive validation interface with:\n",
    "\n",
    "- **Status Progress Bar**: Visual indication of validation progress (0-100%)\n",
    "- **Real-Time Status Display**: Shows current operation, progress percentage, and elapsed time\n",
    "- **Detailed Logs Section**: Timestamped execution logs with emoji indicators:\n",
    "  - ‚ÑπÔ∏è Information messages\n",
    "  - ‚úÖ Success messages\n",
    "  - ‚ö†Ô∏è Warning messages\n",
    "  - ‚ùå Error messages (with full tracebacks)\n",
    "- **Time Tracking**: Automatic tracking of execution time for all validation operations\n",
    "- **Error Handling**: Comprehensive error messages displayed in both the logs and status sections\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- **Regular Benchmarking**: Benchmark operations regularly to track performance over time\n",
    "- **Multiple Validation Methods**: Use multiple validation approaches for robust verification\n",
    "- **Statistical Significance**: Always test statistical significance when comparing methods\n",
    "- **Document Results**: Keep detailed validation reports for reproducibility (use the Export button)\n",
    "- **Set Thresholds**: Define clear acceptance criteria (correlation, error tolerance, p-values)\n",
    "- **Monitor Logs**: Check the logs section for detailed execution information and any warnings\n",
    "- **Review Progress**: Use the status bar to monitor long-running validation operations\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore the validation module API for advanced use cases\n",
    "- Integrate validation into your analysis workflows\n",
    "- Customize validation thresholds for your specific requirements\n",
    "- Review validation logs to optimize performance and identify issues\n",
    "- Export validation reports for documentation and sharing\n",
    "- Contribute validation results to framework documentation\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- Validation Module Documentation: `src/am_qadf/validation/`\n",
    "- Test Examples: `tests/unit/validation/` and `tests/integration/validation/`\n",
    "- Benchmarking Guide: `implementation_plans/BENCHMARKING_USAGE_GUIDE.md`\n",
    "- Validation Test Plan: `implementation_plans/VALIDATION_TEST_PLAN.md`\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've completed the Validation and Benchmarking notebook. You now have the tools to validate framework accuracy, benchmark performance, and ensure reliability in your AM-QADF workflows. The real-time progress tracking and detailed logging features help you monitor and troubleshoot validation operations effectively. üéâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
