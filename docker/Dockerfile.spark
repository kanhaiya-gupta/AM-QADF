# Apache Spark Dockerfile for AM-QADF Framework
# Used for distributed signal mapping execution
FROM apache/spark:3.5.0-scala2.12-java11-python3-ubuntu

# Switch to root for system setup
USER root

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3-pip \
    python3-dev \
    build-essential \
    curl \
    wget \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /opt/spark

# Copy requirements and install Python dependencies
# Note: Adjust paths based on your project structure
COPY requirements.txt /opt/spark/requirements.txt
RUN pip3 install --no-cache-dir -r requirements.txt || \
    (echo "requirements.txt not found, installing minimal dependencies" && \
     pip3 install --no-cache-dir pyspark numpy scipy)

# Copy AM-QADF source code for signal mapping
COPY src/am_qadf /opt/spark/src/am_qadf
COPY src/infrastructure /opt/spark/src/infrastructure
COPY config /opt/spark/config

# Set environment variables
ENV PYTHONPATH="/opt/spark/src:/opt/spark/config:${PYTHONPATH}"
ENV SPARK_HOME="/opt/spark"
ENV PYSPARK_PYTHON="python3"
ENV PYSPARK_DRIVER_PYTHON="python3"

# Create necessary directories
RUN mkdir -p /opt/spark/logs /opt/spark/data /opt/spark/tmp && \
    chown -R spark:spark /opt/spark

# Switch back to spark user
USER spark

# Expose Spark UI port
EXPOSE 4040

# Default command - Spark master
CMD ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master", "--host", "0.0.0.0", "--port", "7077", "--webui-port", "4040"]
